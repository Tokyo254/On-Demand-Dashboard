Product_Name
product_names
product_prices
products
url <- "https://naivas.online/food-cupboard"
page <- read_html(url)
products <- page %>%
html_nodes(".product-list")
product_names <- products %>%
html_node(".product-name") %>%
html_text() %>%
trimws()
product_prices <- products %>%
html_node(".product-price") %>%
html_text() %>%
gsub("Kshs", "", .) %>%
trimws() %>%
as.numeric()
product_data <- data.frame(Product_Name = product_names,
Price_Kshs = product_prices,
stringsAsFactors = FALSE)
print(product_data)
products
product_names
product_prices
product_data
library(rvest)
# specify URL of ecommerce website
url <- "https://naivas.online/food-cupboard"
# send HTTP request to website and parse HTML content
page <- read_html(url)
# extract product names
product_names <- page %>%
html_nodes(".product-list") %>%
html_text()
# extract product prices
product_prices <- page %>%
html_nodes(".product-list") %>%
html_text()
# clean and preprocess product prices
product_prices <- gsub("[^0-9.]", "", product_prices) # remove non-numeric characters
product_prices <- as.numeric(product_prices) # convert to numeric format
product_names
url <- "https://naivas.online/food-cupboard"
page <- read_html(url)
page
product_names
product_names <- products %>%
html_node(".product-name") %>%
html_text() %>%
trimws()
product_names
product_names <- products %>%
html_node(".product-name") %>%
html_text()
product_names
url <- "https://naivas.online/food-cupboard"
page <- read_html(url)
products <- page %>%
html_nodes(".product-list")
product_names <- products %>%
html_node(".product-name") %>%
html_text()
product_names
product_names
url <- "https://naivas.online/food-cupboard"
# send HTTP request to website and parse HTML content
page <- read_html(url)
# extract product names
product_names <- page %>%
html_nodes(".product-list") %>%
html_text()
product_names
product_prices <- page %>%
html_nodes(".product-list") %>%
html_text()
product_prices
# clean and preprocess product prices
product_prices <- gsub("[^0-9.]", "", product_prices) # remove non-numeric characters
product_prices
product_prices <- page %>%
html_nodes(".product-list") %>%
html_text()
product_prices1
product_prices1 <- gsub("[^0-9.]", "", product_prices) # remove non-numeric characters
product_prices1
product_prices
# specify URL of ecommerce website
url <- "https://naivas.online/food-cupboard"
# send HTTP request to website and parse HTML content
page <- read_html(url)
# Find the meta tag with the content attribute "N009240"
meta_tag <- html_nodes(page, xpath = "//meta[@content='N009240']")
# Get the parent element of the meta tag
parent_element <- html_parent(meta_tag)
meta_tag
library(rvest)
# specify URL of ecommerce website
url <- "https://naivas.online/food-cupboard"
# send HTTP request to website and parse HTML content
page <- read_html(url)
# Find the meta tag with the content attribute "N009240"
meta_tag <- html_nodes(page, xpath = "//meta[@content='N009240']")
# Get the parent element of the meta tag
parent_element <- html_parent(meta_tag)
install.packages("xml2")
library(xml2)
library(XML)
library(xmlparsedata)
url <- "https://naivas.online/food-cupboard"
# send HTTP request to website and parse HTML content
page <- read_html(url)
# Find the meta tag with the content attribute "N009240"
meta_tag <- html_nodes(page, xpath = "//meta[@content='N009240']")
# Get the parent element of the meta tag
parent_element <- html_parent(meta_tag)
library(rvest)
library(xml2)
# specify URL of ecommerce website
url <- "https://naivas.online/food-cupboard"
# send HTTP request to website and parse HTML content
page <- read_html(url)
# Find the meta tag with the content attribute "N009240"
meta_tag <- html_nodes(page, xpath = "//meta[@content='N009240']")
# Get the parent element of the meta tag
parent_element <- html_parent(meta_tag)
url <- "https://naivas.online/food-cupboard"
# send HTTP request to website and parse HTML content
page <- read_html(url)
# Find the meta tag with the content attribute "N009240"
meta_tag <- html_nodes(page, xpath = "//meta[@content='N009240']")
# Get the parent element of the meta tag
parent_element <- xml_parent(meta_tag)
# Extract the contents of the parent element
contents <- xml_text(parent_element)
# Print the scraped contents
print(contents)
print(contents)
url <- "https://naivas.online/food-cupboard"
# send HTTP request to website and parse HTML content
page <- read_html(url)
# Find all the contents within the meta tag
meta_tag <- html_nodes(page, xpath = "//meta[@content]")
# G
meta_tag
# Get the parent element of the meta tag
parent_element <- xml_parent(meta_tag)
parent_element
# Extract the contents of the parent element
contents <- xml_text(parent_element)
contents
# Print the scraped contents
print(contents)
library(rvest)
# Define the URL of the website to scrape
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=maize+meal"
# Send a GET request to the URL and read the HTML content
page <- read_html(url)
# Extract the desired information using CSS selectors
product_names <- page %>% html_nodes(".product-name") %>% html_text()
product_availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
product_descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>% html_nodes(".product-price") %>% html_text()
# Print the extracted data
cat("Product Names:\n")
cat(product_names, "\n")
cat("Product Availabilities:\n")
cat(product_availabilities, "\n")
cat("Product Descriptions:\n")
cat(product_descriptions, "\n")
cat("Prices:\n")
cat(prices, "\n")
# Create a dataframe
df <- data.frame(
ProductName = product_names,
Availability = product_availabilities,
Description = product_descriptions,
Price = prices
)
# Print the dataframe
print(df)
Create a dataframe
Flour <- data.frame(
ProductName = product_names,
Availability = product_availabilities,
Description = product_descriptions,
Price = prices
)
# Print the dataframe
print(Flour)
View(df)
# Define the URL of the website to scrape
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=flour"
# Send a GET request to the URL and read the HTML content
page <- read_html(url)
# Extract the desired information using CSS selectors
product_names <- page %>% html_nodes(".product-name") %>% html_text()
product_availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
product_descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>% html_nodes(".product-price") %>% html_text()
# Print the extracted data
cat("Product Names:\n")
cat(product_names, "\n")
cat("Product Availabilities:\n")
cat(product_availabilities, "\n")
cat("Product Descriptions:\n")
cat(product_descriptions, "\n")
cat("Prices:\n")
cat(prices, "\n")
# Create a dataframe
Flour <- data.frame(
ProductName = product_names,
Availability = product_availabilities,
Description = product_descriptions,
Price = prices
)
# Print the dataframe
print(Flour)
Flour
install.packages("RMySQL")
library(RMySQL)
# Establish a connection to the PHPMyAdmin database
con <- dbConnect(MySQL(), user = "root", password = "", dbname = "price_db", host = "localhost")
# Create the table
dbSendQuery(con, "CREATE TABLE scraped_data (ProductName VARCHAR(100), Availability VARCHAR(50), Description VARCHAR(500), Price VARCHAR(20))")
# Define the URL of the website to scrape
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=maize+meal"
# Send a GET request to the URL and read the HTML content
page <- read_html(url)
# Extract the desired information using CSS selectors
product_names <- page %>% html_nodes(".product-name") %>% html_text()
product_availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
product_descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>% html_nodes(".product-price") %>% html_text()
# Insert data into the database table
for (i in 1:length(product_names)) {
query <- paste0("INSERT INTO scraped_data (ProductName, Availability, Description, Price) VALUES ('", product_names[i], "', '", product_availabilities[i], "', '", product_descriptions[i], "', '", prices[i], "')")
dbSendQuery(con, query)
}
# Close the database connection
dbDisconnect(con)
# Establish a connection to the PHPMyAdmin database
con <- dbConnect(MySQL(), user = "root", password = "", dbname = "price_db", host = "localhost")
# Create the table
dbSendQuery(con, "CREATE TABLE Naivas (ProductName VARCHAR(100), Availability VARCHAR(50), Description VARCHAR(500), Price VARCHAR(20))")
# Define the URL of the website to scrape
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=maize+meal"
# Send a GET request to the URL and read the HTML content
page <- read_html(url)
# Extract the desired information using CSS selectors
product_names <- page %>% html_nodes(".product-name") %>% html_text()
product_availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
product_descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>% html_nodes(".product-price") %>% html_text()
# Insert data into the database table
for (i in 1:length(product_names)) {
query <- paste0("INSERT INTO scraped_data (ProductName, Availability, Description, Price) VALUES ('", product_names[i], "', '", product_availabilities[i], "', '", product_descriptions[i], "', '", prices[i], "')")
dbSendQuery(con, query)
}
# Close the database connection
dbDisconnect(con)
install.packages("cronR")
# Connect to the MySQL database
con <- dbConnect(MySQL(),
user = "root",
password = "",
dbname = "price_db",
host = "localhost",
port = 3306)
# Connect to the MySQL database
con <- dbConnect(MySQL(),
user = "root",
password = "",
dbname = "price_db",
host = "localhost")
# Create a data frame with the scraped data
scraped_data <- data.frame(
ProductName = product_names,
Availability = availabilities,
Description = descriptions,
Price = prices
)
# Create a data frame with the scraped data
scraped_data <- data.frame(
ProductName = product_names,
Availability = availabilities,
Description = descriptions,
Price = prices
)
Price
# Establish a connection to the PHPMyAdmin database
con <- dbConnect(MySQL(), user = "root", password = "", dbname = "price_db", host = "localhost")
# Create the table
dbSendQuery(con, "CREATE TABLE Naivas (ProductName VARCHAR(100), Availability VARCHAR(50), Description VARCHAR(500), Price VARCHAR(20))")
# Establish a connection to the PHPMyAdmin database
con <- dbConnect(MySQL(), user = "root", password = "", dbname = "price_db", host = "localhost")
# Create the table
dbSendQuery(con, "CREATE TABLE Naivas2 (ProductName VARCHAR(100), Availability VARCHAR(50), Description VARCHAR(500), Price VARCHAR(20))")
# Define the URL of the website to scrape
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=maize+meal"
# Send a GET request to the URL and read the HTML content
page <- read_html(url)
# Extract the desired information using CSS selectors
product_names <- page %>% html_nodes(".product-name") %>% html_text()
product_availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
product_descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>% html_nodes(".product-price") %>% html_text()
# Insert data into the database table
for (i in 1:length(product_names)) {
query <- paste0("INSERT INTO scraped_data (ProductName, Availability, Description, Price) VALUES ('", product_names[i], "', '", product_availabilities[i], "', '", product_descriptions[i], "', '", prices[i], "')")
dbSendQuery(con, query)
}
# Establish a connection to the PHPMyAdmin database
con <- dbConnect(MySQL(), user = "root", password = "", dbname = "price_db", host = "localhost")
# Create the table
dbSendQuery(con, "CREATE TABLE Naivas2 (ProductName VARCHAR(100), Availability VARCHAR(50), Description VARCHAR(500), Price VARCHAR(20))")
# Establish a connection to the PHPMyAdmin database
con <- dbConnect(MySQL(), user = "root", password = "", dbname = "price_db", host = "localhost")
# Create the table
dbSendQuery(con, "CREATE TABLE Naivas3 (ProductName VARCHAR(100), Availability VARCHAR(50), Description VARCHAR(500), Price VARCHAR(20))")
# Define the URL of the website to scrape
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=maize+meal"
# Send a GET request to the URL and read the HTML content
page <- read_html(url)
# Extract the desired information using CSS selectors
product_names <- page %>% html_nodes(".product-name") %>% html_text()
product_availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
product_descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>% html_nodes(".product-price") %>% html_text()
# Insert data into the database table
for (i in 1:length(product_names)) {
query <- paste0("INSERT INTO Naivas3 (ProductName, Availability, Description, Price) VALUES ('", product_names[i], "', '", product_availabilities[i], "', '", product_descriptions[i], "', '", prices[i], "')")
dbSendQuery(con, query)
}
# Close the database connection
dbDisconnect(con)
availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
scraped_data
availabilities
product_names
descriptions
descriptions
prices
prices
# Scrape data from the website
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=maize+meal"
library(rvest)
library(RMySQL)
# Scrape data from the website
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=maize+meal"
page <- read_html(url)
# Extract relevant data using CSS selectors
product_names <- page %>%
html_nodes(".product-name") %>%
html_text()
availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>%
html_nodes(".price.product-price") %>%
html_text()
# Create a data frame with the scraped data
scraped_data <- data.frame(
ProductName = product_names,
Availability = availabilities,
Description = descriptions,
Price = prices
)
# Connect to the MySQL database
con <- dbConnect(MySQL(),
user = "root",
password = "",
dbname = "price_db",
host = "localhost")
# Insert data into the MySQL database table
dbWriteTable(con, "naivas", scraped_data, overwrite = TRUE)
# Close the database connection
dbDisconnect(con)
con <- dbConnect(MySQL(), user = "root", password = "", dbname = "price_db", host = "localhost")
# Create the table
dbSendQuery(con, "CREATE TABLE Naivas4 (ProductName VARCHAR(100), Availability VARCHAR(50), Description VARCHAR(500), Price VARCHAR(20))")
# Define the URL of the website to scrape
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=maize+meal"
# Send a GET request to the URL and read the HTML content
page <- read_html(url)
# Extract the desired information using CSS selectors
product_names <- page %>% html_nodes(".product-name") %>% html_text()
product_availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
product_descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>% html_nodes(".product-price") %>% html_text()
# Insert data into the database table
for (i in 1:length(product_names)) {
query <- paste0("INSERT INTO Naivas4 (ProductName, Availability, Description, Price) VALUES ('", product_names[i], "', '", product_availabilities[i], "', '", product_descriptions[i], "', '", prices[i], "')")
dbSendQuery(con, query)
}
# Close the database connection
dbDisconnect(con)
library(rvest)
library(RMySQL)
# Scrape data from the website
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=maize+meal"
page <- read_html(url)
# Extract relevant data using CSS selectors
product_names <- page %>%
html_nodes(".product-name") %>%
html_text()
availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>%
html_nodes(".price.product-price") %>%
html_text()
# Create a data frame with the scraped data
scraped_data <- data.frame(
ProductName = product_names,
Availability = availabilities,
Description = descriptions,
Price = prices
)
# Connect to the MySQL database
con <- dbConnect(MySQL(),
user = "root",
password = "",
dbname = "price_db",
host = "localhost")
# Insert data into the MySQL database table
dbWriteTable(con, "naivas", scraped_data, overwrite = TRUE)
# Close the database connection
dbDisconnect(con)
Rscript task_schedule.R
url <- "https://naivas.online/thika-town/"
page <- read_html(url)
# Extract relevant data using CSS selectors
product_names <- page %>%
html_nodes(".product-name") %>%
html_text()
availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>%
html_nodes(".price.product-price") %>%
html_text()
# Create a data frame with the scraped data
scraped_data <- data.frame(
ProductName = product_names,
Availability = availabilities,
Description = descriptions,
Price = prices
)
product_names
prices
scraped_data
url <- "https://naivas.online/thika-town/"
page <- read_html(url)
# Extract relevant data using CSS selectors
product_names <- page %>%
html_nodes(".product-name") %>%
html_text()
availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>%
html_nodes(".price.product-price") %>%
html_text()
scraped_data <- data.frame(
ProductName = product_names,
Availability = availabilities,
Description = descriptions,
Price = prices
)
scraped_data <- data.frame(
ProductName = product_names,
Price = prices
)
# Scrape data from the website
url <- "https://naivas.online/module/ambjolisearch/jolisearch?s=maize+meal"
page <- read_html(url)
# Extract relevant data using CSS selectors
product_names <- page %>%
html_nodes(".product-name") %>%
html_text()
availabilities <- page %>% html_nodes(".product-availability") %>% html_text()
descriptions <- page %>% html_nodes(".product-description-short") %>% html_text()
prices <- page %>%
html_nodes(".price.product-price") %>%
html_text()
# Create a data frame with the scraped data
scraped_data <- data.frame(
ProductName = product_names,
Availability = availabilities,
Description = descriptions,
Price = prices
)
# Scrape data from the website
url <- "https://www.carrefour.ke/mafken/en/v4/search?keyword=maize%20meal"
page <- read_html(url)
# Scrape data from the website
url <- "https://www.carrefour.ke/mafken/en/v4/search?keyword=maize%20meal"
page <- read_html(url)
prices
Price
prices
# Scrape data from the website
url <- "https://www.carrefour.ke/mafken/en/v4/search?keyword=maize%20meal"
page <- read_html(url)
# Extract product names
names <- page %>% html_nodes("[data-testid='product_name']") %>% html_text()
names
prices
url <- "https://www.carrefour.ke/mafken/en/v4/search?keyword=maize%20meal"
page <- read_html(url)
url <- "https://www.carrefour.ke/mafken/en/v4/search?keyword=maize%20meal"
page <- read_html(url)
url <- "https://www.carrefour.ke/mafken/en/v4/search?keyword=maize%20meal"
page <- read_html(url)
setwd("~/Skymart")
